{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Benchmarking QSiLUApprox in YOLOv5\n",
                "**Objective:** Evaluate the accuracy impact of replacing SiLU activation with QSiLUApprox in YOLOv5.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import necessary libraries\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "from ultralytics import YOLO\n",
                "\n",
                "# Custom modules\n",
                "from quantization.quantization_tools import QuantizeActivation, test_quantization, get_qstat\n",
                "from approximation.act_approximation_tools import SiluApproximation, test_silu_approximation\n",
                "from QSiLUApprox.QSiLUApprox import QSiLUApprox\n",
                "from utils.module_replacer import replace_module\n",
                "\n",
                "# Check for GPU availability\n",
                "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load YOLOv5 Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the pre-trained YOLOv5n model\n",
                "model = YOLO('yolov5n.pt').eval().to(device)\n",
                "\n",
                "# Run baseline evaluation (without modification)\n",
                "print(\"\\n--- Evaluating Original Model ---\\n\")\n",
                "baseline_results = model.val(\n",
                "    data='coco.yaml',\n",
                "    batch=32,\n",
                "    imgsz=640,\n",
                "    device=device,\n",
                "    half=True,\n",
                "    workers=14\n",
                ")\n",
                "\n",
                "# Store baseline metrics\n",
                "baseline_map50 = baseline_results.box.map50\n",
                "baseline_map = baseline_results.box.map\n",
                "baseline_latency = baseline_results.speed[\"inference\"]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Replace SiLU with QSiLUApprox"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n--- Replacing SiLU with QSiLUApprox ---\\n\")\n",
                "replace_module(model.model, nn.SiLU, QSiLUApprox, [\"act\"])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Evaluate Modified Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n--- Evaluating Modified Model ---\\n\")\n",
                "modified_results = model.val(\n",
                "    data='coco.yaml',\n",
                "    batch=32,\n",
                "    imgsz=640,\n",
                "    device=device,\n",
                "    half=True,\n",
                "    workers=14\n",
                ")\n",
                "\n",
                "# Store modified model metrics\n",
                "modified_map50 = modified_results.box.map50\n",
                "modified_map = modified_results.box.map\n",
                "modified_latency = modified_results.speed[\"inference\"]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Results Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n--- Benchmark Results ---\\n\")\n",
                "print(f\"mAP@0.5 (Baseline): {baseline_map50:.4f}\")\n",
                "print(f\"mAP@0.5 (Modified): {modified_map50:.4f}\")\n",
                "print(f\"mAP@0.5:0.95 (Baseline): {baseline_map:.4f}\")\n",
                "print(f\"mAP@0.5:0.95 (Modified): {modified_map:.4f}\")\n",
                "print(f\"Inference Latency (Baseline): {baseline_latency:.2f} ms\")\n",
                "print(f\"Inference Latency (Modified): {modified_latency:.2f} ms\")\n",
                "\n",
                "# Plot results\n",
                "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
                "\n",
                "# mAP comparison\n",
                "ax[0].bar([\"Baseline\", \"Modified\"], [baseline_map50, modified_map50], color=['blue', 'orange'])\n",
                "ax[0].set_title(\"mAP@0.5 Comparison\")\n",
                "ax[0].set_ylabel(\"mAP@0.5\")\n",
                "\n",
                "# Inference time comparison\n",
                "ax[1].bar([\"Baseline\", \"Modified\"], [baseline_latency, modified_latency], color=['blue', 'orange'])\n",
                "ax[1].set_title(\"Inference Latency Comparison\")\n",
                "ax[1].set_ylabel(\"Latency (ms)\")\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conclusion"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n--- Summary ---\\n\")\n",
                "if modified_map50 >= baseline_map50:\n",
                "    print(\"\u2705 The modified model maintains or improves accuracy.\")\n",
                "else:\n",
                "    print(\"\u26a0\ufe0f The modified model has a slight accuracy drop.\")\n",
                "\n",
                "if modified_latency <= baseline_latency:\n",
                "    print(\"\u2705 The modified model is more efficient.\")\n",
                "else:\n",
                "    print(\"\u26a0\ufe0f The modified model has increased latency.\")\n",
                "\n",
                "print(\"\\nFurther optimizations may be needed for better trade-offs between accuracy and efficiency.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}